# Selective Forgetting-Aware Optimizer 
SFAO uses gradient-subspace cosine similarity toselectively determine whether to project, accept, or discard each update, with a per-layer gating mechanism that can be tuned and efficiently approximated via Monte Carlo sampling. This design enables fine-grained control over the degree of alignment with past tasks while reducing unnecessary computation. Experiments on standard continual learning benchmarks demonstrate that SFAO improves accuracy, mitigates forgetting, and lowers energy consumption.

## Abstract 
As neural networks are increasingly deployed in dynamic environments, the ability to learn sequentially without forgetting has become more important than ever. Neural networks, however, suffer from catastrophic forgetting (CF), the tendency to lose previously acquired knowledge when adapting to new tasks, despite having the capacity to solve both old and new tasks if trained jointly. This limitation hinders the LLMâ€™s robustness and its ability to prevent erroneous outputs from occurring. In this paper, we address this challenge from a parameter space perspective and introduce Selective Forgetting-Aware Optimization (SFAO), a dynamic approach that regulates the direction of gradient updates to preserve relevant prior knowledge while enabling efficient adaptation. SFAO uses gradient-subspace cosine similarity to selectively determine whether to project, accept, or discard each update, with a per-layer gating mechanism that can be tuned and efficiently approximated via Monte Carlo sampling. This design enables fine-grained control over the degree of alignment with past tasks while reducing unnecessary computation. Experiments on standard continual learning benchmarks demonstrate that SFAO improves robustness, accuracy, mitigates forgetting, and lowers memory consumption.
