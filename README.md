<div align="center">
    
# Mitigating Forgetting w/ SFAO
SFAO uses gradient-subspace cosine similarity toselectively determine whether to project, accept, or discard each update, with a per-layer gating mechanism that can be tuned and efficiently approximated via Monte Carlo sampling. This design enables fine-grained control over the degree of alignment with past tasks while reducing unnecessary computation. Experiments on standard continual learning benchmarks demonstrate that SFAO improves accuracy, mitigates forgetting, and lowers energy consumption.

</div>
