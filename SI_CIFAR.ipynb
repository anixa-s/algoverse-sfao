{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dkauMCEMJFT"
   },
   "outputs": [],
   "source": [
    "# !ls -R /content/wide-resnet.pytorch | sed -n '1,200p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CiS38e4NSh5",
    "outputId": "8e26fd0d-b40b-4b32-bb96-a9677f3173c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/wide-resnet.pytorch'...\n",
      "remote: Enumerating objects: 124, done.\u001b[K\n",
      "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 124 (delta 29), reused 29 (delta 29), pack-reused 89 (from 1)\u001b[K\n",
      "Receiving objects: 100% (124/124), 670.63 KiB | 8.71 MiB/s, done.\n",
      "Resolving deltas: 100% (66/66), done.\n",
      "Top-level entries: ['.git', '.gitignore', 'INSTALL.md', 'LICENSE', 'README.md', 'SERVER.md', 'config.py', 'imgs', 'main.py', 'networks', 'scripts']\n",
      "total 48\n",
      "drwxr-xr-x 6 root root 4096 Aug 24 18:22 .\n",
      "drwxr-xr-x 3 root root   41 Aug 24 18:22 ..\n",
      "drwxr-xr-x 8 root root 4096 Aug 24 18:22 .git\n",
      "-rw-r--r-- 1 root root 1185 Aug 24 18:22 .gitignore\n",
      "-rw-r--r-- 1 root root 4999 Aug 24 18:22 INSTALL.md\n",
      "-rw-r--r-- 1 root root 1067 Aug 24 18:22 LICENSE\n",
      "-rw-r--r-- 1 root root 3619 Aug 24 18:22 README.md\n",
      "-rw-r--r-- 1 root root 2819 Aug 24 18:22 SERVER.md\n",
      "-rw-r--r-- 1 root root  787 Aug 24 18:22 config.py\n",
      "drwxr-xr-x 2 root root  141 Aug 24 18:22 imgs\n",
      "-rw-r--r-- 1 root root 8463 Aug 24 18:22 main.py\n",
      "drwxr-xr-x 2 root root  121 Aug 24 18:22 networks\n",
      "drwxr-xr-x 2 root root  103 Aug 24 18:22 scripts\n",
      "---\n",
      "/content/wide-resnet.pytorch:\n",
      "INSTALL.md\n",
      "LICENSE\n",
      "README.md\n",
      "SERVER.md\n",
      "config.py\n",
      "imgs\n",
      "main.py\n",
      "networks\n",
      "scripts\n",
      "\n",
      "/content/wide-resnet.pytorch/imgs:\n",
      "cifar100_image.png\n",
      "cifar10_image.png\n",
      "img_356.lua\n",
      "pytorch.png\n",
      "svhn_image.png\n",
      "\n",
      "/content/wide-resnet.pytorch/networks:\n",
      "__init__.py\n",
      "lenet.py\n",
      "resnet.py\n",
      "vggnet.py\n",
      "wide_resnet.py\n",
      "\n",
      "/content/wide-resnet.pytorch/scripts:\n",
      "cifar100_train.sh\n",
      "cifar10_train.sh\n",
      "resnet_cifar100_train.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean any previous clones\n",
    "import shutil, os, sys, subprocess, textwrap\n",
    "\n",
    "REPO_DIR = \"/content/wide-resnet.pytorch\"\n",
    "if os.path.isdir(REPO_DIR):\n",
    "    print(\"Removing old repo at\", REPO_DIR)\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Fresh clone\n",
    "!git clone https://github.com/bmsookim/wide-resnet.pytorch.git /content/wide-resnet.pytorch\n",
    "\n",
    "# Verify the clone really exists and has files\n",
    "import os, itertools\n",
    "assert os.path.isdir(REPO_DIR), f\"Repo not found at {REPO_DIR}\"\n",
    "top = os.listdir(REPO_DIR)\n",
    "print(\"Top-level entries:\", top[:50])\n",
    "\n",
    "# Print first ~200 lines of the tree\n",
    "print(subprocess.run(\n",
    "    [\"bash\",\"-lc\",f\"ls -la {REPO_DIR}; echo '---'; ls -R {REPO_DIR} | sed -n '1,200p'\"],\n",
    "    capture_output=True, text=True\n",
    ").stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-6MthDyINZp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/wide-resnet.pytorch:\n",
      "INSTALL.md\n",
      "LICENSE\n",
      "README.md\n",
      "SERVER.md\n",
      "config.py\n",
      "imgs\n",
      "main.py\n",
      "networks\n",
      "scripts\n",
      "\n",
      "/content/wide-resnet.pytorch/imgs:\n",
      "cifar100_image.png\n",
      "cifar10_image.png\n",
      "img_356.lua\n",
      "pytorch.png\n",
      "svhn_image.png\n",
      "\n",
      "/content/wide-resnet.pytorch/networks:\n",
      "__init__.py\n",
      "lenet.py\n",
      "resnet.py\n",
      "vggnet.py\n",
      "wide_resnet.py\n",
      "\n",
      "/content/wide-resnet.pytorch/scripts:\n",
      "cifar100_train.sh\n",
      "cifar10_train.sh\n",
      "resnet_cifar100_train.sh\n"
     ]
    }
   ],
   "source": [
    "!ls -R /content/wide-resnet.pytorch | sed -n '1,200p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-lRUAziN9jN",
    "outputId": "9226dc4d-3e5c-4a0b-bcaa-a85b0dda1b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:def conv3x3(in_planes, out_planes, stride=1):\n",
      "13:def conv_init(m):\n",
      "22:class wide_basic(nn.Module):\n",
      "23:    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
      "37:    def forward(self, x):\n",
      "44:class Wide_ResNet(nn.Module):\n",
      "45:    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
      "63:    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
      "73:    def forward(self, x):\n"
     ]
    }
   ],
   "source": [
    "!grep -nE \"class |def \" /content/wide-resnet.pytorch/networks/wide_resnet.py | head -n 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHrParIjOJIR",
    "outputId": "97faa722-0568-429a-d879-3342edfd0bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Wide-Resnet 28x10\n",
      "✅ Model defined: Wide_ResNet on cuda:0\n",
      "✅ Forward OK — output shape: (2, 100)\n"
     ]
    }
   ],
   "source": [
    "# Import & define the model from this repo variant\n",
    "import sys, torch\n",
    "sys.path.append('/content/wide-resnet.pytorch')\n",
    "\n",
    "from networks.wide_resnet import Wide_ResNet  # ← underscore!\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 100\n",
    "\n",
    "# Constructor: (depth, widen_factor, dropout_rate, num_classes)\n",
    "model = Wide_ResNet(depth=28, widen_factor=10, dropout_rate=0.0, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "print(\"✅ Model defined:\", type(model).__name__, \"on\", next(model.parameters()).device)\n",
    "\n",
    "# Quick forward sanity check (CIFAR-sized input 3x32x32)\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(2, 3, 32, 32, device=device)\n",
    "    y = model(x)\n",
    "    print(\"✅ Forward OK — output shape:\", tuple(y.shape))\n",
    "    assert y.shape[1] == NUM_CLASSES, \"Output classes mismatch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ztIP_tVdVKW",
    "outputId": "a7732bbe-289e-4255-cac6-38dba187ca81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     subset \u001b[38;5;241m=\u001b[39m Subset(dataset, idx)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(subset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m train_loaders \u001b[38;5;241m=\u001b[39m [make_loader(train_dataset, cls_ids, BATCH_SIZE, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m cls_ids \u001b[38;5;129;01min\u001b[39;00m class_splits]\n\u001b[1;32m     51\u001b[0m test_loaders \u001b[38;5;241m=\u001b[39m [make_loader(test_dataset, cls_ids, BATCH_SIZE, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m cls_ids \u001b[38;5;129;01min\u001b[39;00m class_splits]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Created \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loaders)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train loaders and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_loaders)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test loaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     subset \u001b[38;5;241m=\u001b[39m Subset(dataset, idx)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(subset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m train_loaders \u001b[38;5;241m=\u001b[39m [\u001b[43mmake_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cls_ids \u001b[38;5;129;01min\u001b[39;00m class_splits]\n\u001b[1;32m     51\u001b[0m test_loaders \u001b[38;5;241m=\u001b[39m [make_loader(test_dataset, cls_ids, BATCH_SIZE, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m cls_ids \u001b[38;5;129;01min\u001b[39;00m class_splits]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Created \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loaders)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train loaders and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_loaders)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test loaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 46\u001b[0m, in \u001b[0;36mmake_loader\u001b[0;34m(dataset, class_ids, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_loader\u001b[39m(dataset, class_ids, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 46\u001b[0m     idx \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, (_, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset) \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m class_ids]\n\u001b[1;32m     47\u001b[0m     subset \u001b[38;5;241m=\u001b[39m Subset(dataset, idx)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(subset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_loader\u001b[39m(dataset, class_ids, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 46\u001b[0m     idx \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, (_, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset) \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m class_ids]\n\u001b[1;32m     47\u001b[0m     subset \u001b[38;5;241m=\u001b[39m Subset(dataset, idx)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(subset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:688\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    685\u001b[0m     padding \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m height]\n\u001b[1;32m    686\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(img, padding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode)\n\u001b[0;32m--> 688\u001b[0m i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcrop(img, i, j, h, w)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:653\u001b[0m, in \u001b[0;36mRandomCrop.get_params\u001b[0;34m(img, output_size)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, h, w\n\u001b[1;32m    652\u001b[0m i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, h \u001b[38;5;241m-\u001b[39m th \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 653\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m i, j, th, tw\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Wide_ResNet + SI + Replay + KD on Split CIFAR-100\n",
    "# =========================\n",
    "\n",
    "# 0) Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Model import (adjust path to where Wide_ResNet is cloned)\n",
    "sys.path.append('/content/wide-resnet.pytorch')  # adjust if needed\n",
    "from networks.wide_resnet import Wide_ResNet\n",
    "\n",
    "# 2) Device and constants\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 100  # CIFAR-100\n",
    "NUM_TASKS = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2  # increase for real training\n",
    "MEMORY_MAX_PER_CLASS = 10  # replay buffer size per class\n",
    "\n",
    "# 3) Data preparation: Split CIFAR-100 into 10 disjoint tasks\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409),\n",
    "                         (0.2673, 0.2564, 0.2762))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "classes_per_task = NUM_CLASSES // NUM_TASKS\n",
    "class_splits = [list(range(i * classes_per_task, (i + 1) * classes_per_task)) for i in range(NUM_TASKS)]\n",
    "\n",
    "def make_loader(dataset, class_ids, batch_size=32, shuffle=True):\n",
    "    idx = [i for i, (_, y) in enumerate(dataset) if y in class_ids]\n",
    "    subset = Subset(dataset, idx)\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "train_loaders = [make_loader(train_dataset, cls_ids, BATCH_SIZE, True) for cls_ids in class_splits]\n",
    "test_loaders = [make_loader(test_dataset, cls_ids, BATCH_SIZE, False) for cls_ids in class_splits]\n",
    "\n",
    "print(f\" Created {len(train_loaders)} train loaders and {len(test_loaders)} test loaders\")\n",
    "\n",
    "# 4) Model definition\n",
    "model = Wide_ResNet(depth=28, widen_factor=10, dropout_rate=0.0, num_classes=NUM_CLASSES).to(device)\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(2, 3, 32, 32, device=device)\n",
    "    y = model(x)\n",
    "    print(\"Forward OK — output shape:\", tuple(y.shape))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# 5) Evaluation helper\n",
    "# Fix evaluate() for Split CIFAR-10\n",
    "def evaluate(model, loader, class_offset):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), (y + class_offset).to(device)\n",
    "            outputs = model(x)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "# 6) Synaptic Intelligence class\n",
    "class SynapticIntelligence:\n",
    "    def __init__(self, model, c=600, xi=0.1, device=None):\n",
    "        self.model = model\n",
    "        self.c = c\n",
    "        self.xi = xi\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self.params = [p for p in model.parameters() if p.requires_grad]\n",
    "        self.omega = [torch.zeros_like(p.data, device=self.device) for p in self.params]\n",
    "        self.theta_ref = [p.data.clone().detach() for p in self.params]\n",
    "        self.w = [torch.zeros_like(p.data, device=self.device) for p in self.params]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def begin_task(self):\n",
    "        for k in range(len(self.params)):\n",
    "            self.w[k].zero_()\n",
    "\n",
    "    def penalty(self):\n",
    "        reg = 0.0\n",
    "        for p, omega_i, theta_star in zip(self.params, self.omega, self.theta_ref):\n",
    "            reg = reg + torch.sum(omega_i * (p - theta_star) ** 2)\n",
    "        return self.c * reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_w_after_step(self, old_params):\n",
    "        for k, p in enumerate(self.params):\n",
    "            if p.grad is None: continue\n",
    "            delta = p.data - old_params[k]\n",
    "            self.w[k].add_((-p.grad).detach() * delta)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def end_task(self):\n",
    "        for k, p in enumerate(self.params):\n",
    "            # denom = (p.data - self.theta_ref[k]) ** 2 + self.xi\n",
    "            denom = torch.clamp((p.data - self.theta_ref[k]) ** 2 + self.xi, min=1e-6)\n",
    "            self.omega[k].add_(self.w[k] / denom)\n",
    "            self.theta_ref[k] = p.data.clone().detach()\n",
    "            self.w[k].zero_()\n",
    "\n",
    "# 7) Replay + KD helpers\n",
    "memory = {}\n",
    "seen_classes = set()\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_memory_from_loader(loader, per_class=MEMORY_MAX_PER_CLASS):\n",
    "    global memory, seen_classes\n",
    "    counts = {}\n",
    "    for xb, yb in loader:\n",
    "        for x, y in zip(xb, yb):\n",
    "            y = int(y.item())\n",
    "            if counts.get(y, 0) < per_class:\n",
    "                memory.setdefault(y, []).append((x.cpu(), y))\n",
    "                counts[y] = counts.get(y, 0) + 1\n",
    "                seen_classes.add(y)\n",
    "\n",
    "def sample_memory(batch_size):\n",
    "    if not memory or not seen_classes:\n",
    "        return None, None\n",
    "    pool = []\n",
    "    for c in seen_classes:\n",
    "        pool.extend(memory.get(c, []))\n",
    "    if not pool:\n",
    "        return None, None\n",
    "    k = min(batch_size, len(pool))\n",
    "    samples = random.sample(pool, k)\n",
    "    X = torch.stack([s[0] for s in samples]).to(device)\n",
    "    y = torch.tensor([s[1] for s in samples], device=device)\n",
    "    return X, y\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, T=2.0):\n",
    "    p_s = F.log_softmax(student_logits / T, dim=1)\n",
    "    p_t = F.softmax(teacher_logits / T, dim=1)\n",
    "    return F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "\n",
    "# 8) SI instantiation\n",
    "si = SynapticIntelligence(model, c=300.0, xi=0.1, device=device)\n",
    "\n",
    "# 9) Training loop\n",
    "results = np.zeros((NUM_TASKS, NUM_TASKS))\n",
    "teacher = None\n",
    "\n",
    "for t in range(NUM_TASKS):\n",
    "    print(f\"\\n=== Starting Task {t + 1}/{NUM_TASKS} ===\")\n",
    "\n",
    "    if t > 0:\n",
    "        teacher = deepcopy(model).to(device)\n",
    "        teacher.eval()\n",
    "        for p in teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    model.train()\n",
    "    si.begin_task()\n",
    "\n",
    "    loader = train_loaders[t]\n",
    "    if len(loader) == 0:\n",
    "        print(f\"train_loaders[{t}] is empty! Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"-- Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(tqdm(loader, desc=f\"Task {t} | Epoch {epoch}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_cur = model(x)\n",
    "            ce_cur = criterion(logits_cur, y)\n",
    "            reg_loss = si.penalty() if t > 0 else 0.0\n",
    "\n",
    "            kd = 0.0\n",
    "            ce_rep = 0.0\n",
    "            if teacher is not None:\n",
    "                xr, yr = sample_memory(batch_size=max(1, BATCH_SIZE // 4))\n",
    "                if xr is not None:\n",
    "                    with torch.no_grad():\n",
    "                        t_logits = teacher(xr)\n",
    "                    s_logits = model(xr)\n",
    "                    ce_rep = criterion(s_logits, yr)\n",
    "                    kd = kd_loss(s_logits, t_logits, T=2.0)\n",
    "            \"\"\"\n",
    "            loss = ce_cur + 0.5 * (ce_rep if isinstance(ce_rep, torch.Tensor) else 0.0) \\\n",
    "                         + 0.5 * (kd if isinstance(kd, torch.Tensor) else 0.0) \\\n",
    "                         + (reg_loss if isinstance(reg_loss, torch.Tensor) else 0.0)\n",
    "            \"\"\"\n",
    "            loss = ce_cur + 0.25 * ce_rep + 0.25 * kd + reg_loss  # lower weights for stability\n",
    "\n",
    "            old_params = [p.data.clone() for p in si.params]\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "            #si.update_w_after_step(old_params)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += float(ce_cur.detach().cpu())\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Batch {i}/{len(loader)} | CE: {ce_cur:.4f} | SI: {reg_loss:.4f} | REP: {float(ce_rep):.4f} | KD: {float(kd):.4f}\")\n",
    "\n",
    "        avg_loss = running_loss / len(loader)\n",
    "        print(f\"Avg CE for Task {t}, Epoch {epoch+1}: {avg_loss:.4f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    si.end_task()\n",
    "    update_memory_from_loader(loader, per_class=MEMORY_MAX_PER_CLASS)\n",
    "\n",
    "    # Eval\n",
    "    for eval_t in range(t + 1):\n",
    "        acc = evaluate(model, test_loaders[eval_t])\n",
    "        results[t][eval_t] = acc\n",
    "        print(f\"[Task {t}] Eval on Task {eval_t}: {acc:.2f}%\")\n",
    "\n",
    "    print(\"\\nAccuracy Summary After Task\", t)\n",
    "    for prev_task in range(t + 1):\n",
    "        print(f\"    Task {prev_task} accuracy: {results[t][prev_task]:.2f}%\")\n",
    "\n",
    "    if t > 0:\n",
    "        print(\"\\nForgetting After Task\", t)\n",
    "        for prev_task in range(t):\n",
    "            acc_before = results[prev_task][prev_task]\n",
    "            acc_now = results[t][prev_task]\n",
    "            forgetting = acc_before - acc_now\n",
    "            print(f\"    Task {prev_task}: Before={acc_before:.2f}% Now={acc_now:.2f}% Forgotten={forgetting:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete. Results matrix:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Created 5 train loaders and 5 test loaders\n",
      "| Wide-Resnet 28x10\n",
      "Forward OK — output shape: (2, 10)\n",
      "\n",
      "=== Starting Task 1/5 ===\n",
      "-- Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0:   1%|          | 3/313 [00:00<00:31,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/313 | CE: 2.2624 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0:  17%|█▋        | 53/313 [00:03<00:15, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/313 | CE: 0.5748 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0:  33%|███▎      | 103/313 [00:06<00:12, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/313 | CE: 0.3596 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0:  49%|████▉     | 153/313 [00:09<00:09, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/313 | CE: 0.5101 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0:  65%|██████▍   | 203/313 [00:12<00:06, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/313 | CE: 0.1791 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0:  81%|████████  | 253/313 [00:15<00:03, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/313 | CE: 0.3499 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0:  97%|█████████▋| 303/313 [00:18<00:00, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/313 | CE: 0.4567 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 0: 100%|██████████| 313/313 [00:18<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg CE for Task 0, Epoch 1: 0.4599\n",
      "-- Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1:   1%|          | 3/313 [00:00<00:31,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/313 | CE: 0.4839 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1:  17%|█▋        | 53/313 [00:03<00:15, 16.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/313 | CE: 0.2943 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1:  33%|███▎      | 103/313 [00:06<00:12, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/313 | CE: 0.1956 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1:  49%|████▉     | 153/313 [00:09<00:09, 16.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/313 | CE: 0.1090 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1:  65%|██████▍   | 203/313 [00:12<00:06, 16.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/313 | CE: 0.3149 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1:  81%|████████  | 253/313 [00:15<00:03, 16.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/313 | CE: 0.3804 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1:  97%|█████████▋| 303/313 [00:18<00:00, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/313 | CE: 0.1911 | SI: 0.0000 | REP: 0.0000 | KD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 0 | Epoch 1: 100%|██████████| 313/313 [00:19<00:00, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg CE for Task 0, Epoch 2: 0.2715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 0] Eval on Task 0: 91.95%\n",
      "\n",
      "Accuracy Summary After Task 0\n",
      "    Task 0: 91.95%\n",
      "\n",
      "=== Starting Task 2/5 ===\n",
      "-- Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0:   1%|          | 2/313 [00:00<00:44,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/313 | CE: 8.3805 | SI: 0.0000 | REP: 0.1859 | KD: 0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0:  17%|█▋        | 52/313 [00:05<00:25, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/313 | CE: 1.3812 | SI: 0.2712 | REP: 0.1747 | KD: 1.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0:  33%|███▎      | 102/313 [00:09<00:19, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/313 | CE: 1.2716 | SI: 0.2200 | REP: 0.2020 | KD: 1.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0:  49%|████▊     | 152/313 [00:14<00:15, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/313 | CE: 0.9403 | SI: 0.2071 | REP: 0.2337 | KD: 0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0:  65%|██████▍   | 202/313 [00:19<00:10, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/313 | CE: 0.8605 | SI: 0.2028 | REP: 0.1358 | KD: 0.8376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0:  81%|████████  | 252/313 [00:24<00:05, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/313 | CE: 0.8224 | SI: 0.2041 | REP: 0.1612 | KD: 1.2673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0:  96%|█████████▋| 302/313 [00:28<00:01, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/313 | CE: 0.8074 | SI: 0.1983 | REP: 0.1433 | KD: 0.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 0: 100%|██████████| 313/313 [00:30<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg CE for Task 1, Epoch 1: 1.4761\n",
      "-- Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1:   1%|          | 2/313 [00:00<01:01,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/313 | CE: 0.7094 | SI: 0.2014 | REP: 0.1552 | KD: 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1:  17%|█▋        | 52/313 [00:05<00:25, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/313 | CE: 0.9154 | SI: 0.2034 | REP: 0.2079 | KD: 0.5132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1:  33%|███▎      | 102/313 [00:10<00:20, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/313 | CE: 0.5696 | SI: 0.2066 | REP: 0.1953 | KD: 0.4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1:  49%|████▊     | 152/313 [00:15<00:15, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/313 | CE: 1.1136 | SI: 0.2102 | REP: 0.2405 | KD: 0.5098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1:  65%|██████▍   | 202/313 [00:19<00:10, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/313 | CE: 0.7563 | SI: 0.2157 | REP: 0.0998 | KD: 0.5285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1:  81%|████████  | 252/313 [00:24<00:05, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/313 | CE: 0.8544 | SI: 0.2345 | REP: 0.0746 | KD: 0.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1:  96%|█████████▋| 302/313 [00:29<00:01, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/313 | CE: 1.1486 | SI: 0.2360 | REP: 0.1740 | KD: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 1 | Epoch 1: 100%|██████████| 313/313 [00:30<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg CE for Task 1, Epoch 2: 0.8007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 1] Eval on Task 0: 58.60%\n",
      "[Task 1] Eval on Task 1: 0.00%\n",
      "\n",
      "Accuracy Summary After Task 1\n",
      "    Task 0: 58.60%\n",
      "    Task 1: 0.00%\n",
      "\n",
      "=== Starting Task 3/5 ===\n",
      "-- Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0:   1%|          | 2/313 [00:00<01:00,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/313 | CE: 6.4262 | SI: 0.0000 | REP: 0.1761 | KD: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0:  17%|█▋        | 52/313 [00:05<00:24, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/313 | CE: 3.4519 | SI: 0.1933 | REP: 1.2444 | KD: 0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0:  33%|███▎      | 102/313 [00:09<00:19, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/313 | CE: 1.7261 | SI: 0.1946 | REP: 0.9797 | KD: 1.9299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0:  49%|████▊     | 152/313 [00:14<00:15, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/313 | CE: 1.3535 | SI: 0.2708 | REP: 0.9413 | KD: 1.8703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0:  65%|██████▍   | 202/313 [00:19<00:10, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/313 | CE: 0.9003 | SI: 0.2881 | REP: 0.6675 | KD: 1.1079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0:  81%|████████  | 252/313 [00:24<00:05, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/313 | CE: 0.9414 | SI: 0.2953 | REP: 0.3751 | KD: 1.5331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0:  96%|█████████▋| 302/313 [00:29<00:01, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/313 | CE: 0.7079 | SI: 0.2975 | REP: 1.0994 | KD: 2.1865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 0: 100%|██████████| 313/313 [00:30<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg CE for Task 2, Epoch 1: 1.8879\n",
      "-- Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1:   1%|          | 2/313 [00:00<00:50,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/313 | CE: 0.5980 | SI: 0.3096 | REP: 0.6367 | KD: 1.5944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1:  17%|█▋        | 53/313 [00:05<00:25, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/313 | CE: 0.6851 | SI: 0.3034 | REP: 0.3211 | KD: 0.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1:  33%|███▎      | 103/313 [00:10<00:19, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/313 | CE: 0.6503 | SI: 0.3140 | REP: 1.6878 | KD: 0.9782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1:  49%|████▉     | 153/313 [00:14<00:14, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/313 | CE: 0.6033 | SI: 0.3174 | REP: 0.2713 | KD: 0.6914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1:  65%|██████▍   | 203/313 [00:19<00:10, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/313 | CE: 2.8719 | SI: 0.3356 | REP: 0.8138 | KD: 0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1:  81%|████████  | 253/313 [00:24<00:05, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/313 | CE: 0.6851 | SI: 0.3282 | REP: 0.7195 | KD: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1:  97%|█████████▋| 303/313 [00:29<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/313 | CE: 3.1574 | SI: 0.3286 | REP: 0.5287 | KD: 0.5442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 2 | Epoch 1: 100%|██████████| 313/313 [00:30<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg CE for Task 2, Epoch 2: 0.7473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 2] Eval on Task 0: 71.95%\n",
      "[Task 2] Eval on Task 1: 6.60%\n",
      "[Task 2] Eval on Task 2: 0.00%\n",
      "\n",
      "Accuracy Summary After Task 2\n",
      "    Task 0: 71.95%\n",
      "    Task 1: 6.60%\n",
      "    Task 2: 0.00%\n",
      "\n",
      "=== Starting Task 4/5 ===\n",
      "-- Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 0:   1%|          | 2/313 [00:00<00:54,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/313 | CE: 5.9930 | SI: 0.0000 | REP: 1.4178 | KD: 0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 0:  17%|█▋        | 52/313 [00:05<00:24, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/313 | CE: 2.8591 | SI: 0.1711 | REP: 1.6523 | KD: 0.2570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 0:  33%|███▎      | 102/313 [00:10<00:20, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/313 | CE: 2.4999 | SI: 0.2115 | REP: 1.3509 | KD: 0.5809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 0:  49%|████▊     | 152/313 [00:14<00:15, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/313 | CE: 1.5841 | SI: 0.2597 | REP: 0.6346 | KD: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task 3 | Epoch 0:  56%|█████▌    | 174/313 [00:17<00:13, 10.37it/s]"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Wide_ResNet + SI + Replay + KD on Split CIFAR-10\n",
    "# =========================\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) WideResNet import (adjust path if needed)\n",
    "sys.path.append('/content/wide-resnet.pytorch')  # path to WRN implementation\n",
    "from networks.wide_resnet import Wide_ResNet\n",
    "\n",
    "# 2) Device and constants\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "NUM_TASKS = 5            # 5 splits → 2 classes each\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2               # increase for better results\n",
    "MEMORY_MAX_PER_CLASS = 10\n",
    "LR = 0.001                \n",
    "SI_C = 600               # strong SI regularization as per your finding\n",
    "\n",
    "# 3) Data preparation: Split CIFAR-10 into 5 disjoint tasks (2 classes each)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "classes_per_task = NUM_CLASSES // NUM_TASKS\n",
    "class_splits = [list(range(i * classes_per_task, (i + 1) * classes_per_task)) for i in range(NUM_TASKS)]\n",
    "\n",
    "def make_loader(dataset, class_ids, batch_size=32, shuffle=True):\n",
    "    idx = [i for i, (_, y) in enumerate(dataset) if y in class_ids]\n",
    "    subset = Subset(dataset, idx)\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "train_loaders = [make_loader(train_dataset, cls_ids, BATCH_SIZE, True) for cls_ids in class_splits]\n",
    "test_loaders = [make_loader(test_dataset, cls_ids, BATCH_SIZE, False) for cls_ids in class_splits]\n",
    "\n",
    "print(f\"Created {len(train_loaders)} train loaders and {len(test_loaders)} test loaders\")\n",
    "\n",
    "# 4) Model definition\n",
    "model = Wide_ResNet(depth=28, widen_factor=10, dropout_rate=0.0, num_classes=NUM_CLASSES).to(device)\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(2, 3, 32, 32, device=device)\n",
    "    y = model(x)\n",
    "    print(\"Forward OK — output shape:\", tuple(y.shape))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# 5) Evaluation helper\n",
    "def evaluate(model, loader, class_offset=0):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), (y + class_offset).to(device)\n",
    "            outputs = model(x)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "# 6) Synaptic Intelligence\n",
    "class SynapticIntelligence:\n",
    "    def __init__(self, model, c=SI_C, xi=0.1, device=None):\n",
    "        self.model = model\n",
    "        self.c = c\n",
    "        self.xi = xi\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self.params = [p for p in model.parameters() if p.requires_grad]\n",
    "        self.omega = [torch.zeros_like(p.data, device=self.device) for p in self.params]\n",
    "        self.theta_ref = [p.data.clone().detach() for p in self.params]\n",
    "        self.w = [torch.zeros_like(p.data, device=self.device) for p in self.params]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def begin_task(self):\n",
    "        for k in range(len(self.params)):\n",
    "            self.w[k].zero_()\n",
    "\n",
    "    def penalty(self):\n",
    "        reg = 0.0\n",
    "        for p, omega_i, theta_star in zip(self.params, self.omega, self.theta_ref):\n",
    "            reg = reg + torch.sum(omega_i * (p - theta_star) ** 2)\n",
    "        return self.c * reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_w_after_step(self, old_params):\n",
    "        for k, p in enumerate(self.params):\n",
    "            if p.grad is None: continue\n",
    "            delta = p.data - old_params[k]\n",
    "            self.w[k].add_((-p.grad).detach() * delta)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def end_task(self):\n",
    "        for k, p in enumerate(self.params):\n",
    "            denom = torch.clamp((p.data - self.theta_ref[k]) ** 2 + self.xi, min=1e-6)\n",
    "            self.omega[k].add_(self.w[k] / denom)\n",
    "            self.theta_ref[k] = p.data.clone().detach()\n",
    "            self.w[k].zero_()\n",
    "\n",
    "# 7) Replay + KD helpers\n",
    "memory = {}\n",
    "seen_classes = set()\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_memory_from_loader(loader, per_class=MEMORY_MAX_PER_CLASS):\n",
    "    global memory, seen_classes\n",
    "    counts = {}\n",
    "    for xb, yb in loader:\n",
    "        for x, y in zip(xb, yb):\n",
    "            y = int(y.item())\n",
    "            if counts.get(y, 0) < per_class:\n",
    "                memory.setdefault(y, []).append((x.cpu(), y))\n",
    "                counts[y] = counts.get(y, 0) + 1\n",
    "                seen_classes.add(y)\n",
    "\n",
    "def sample_memory(batch_size):\n",
    "    if not memory or not seen_classes:\n",
    "        return None, None\n",
    "    pool = []\n",
    "    for c in seen_classes:\n",
    "        pool.extend(memory.get(c, []))\n",
    "    if not pool:\n",
    "        return None, None\n",
    "    k = min(batch_size, len(pool))\n",
    "    samples = random.sample(pool, k)\n",
    "    X = torch.stack([s[0] for s in samples]).to(device)\n",
    "    y = torch.tensor([s[1] for s in samples], device=device)\n",
    "    return X, y\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, T=2.0):\n",
    "    p_s = F.log_softmax(student_logits / T, dim=1)\n",
    "    p_t = F.softmax(teacher_logits / T, dim=1)\n",
    "    return F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n",
    "\n",
    "# 8) SI instantiation\n",
    "si = SynapticIntelligence(model, c=SI_C, xi=0.1, device=device)\n",
    "\n",
    "# 9) Training loop\n",
    "results = np.zeros((NUM_TASKS, NUM_TASKS))\n",
    "teacher = None\n",
    "\n",
    "for t in range(NUM_TASKS):\n",
    "    print(f\"\\n=== Starting Task {t + 1}/{NUM_TASKS} ===\")\n",
    "\n",
    "    if t > 0:\n",
    "        teacher = deepcopy(model).to(device)\n",
    "        teacher.eval()\n",
    "        for p in teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    model.train()\n",
    "    si.begin_task()\n",
    "    loader = train_loaders[t]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"-- Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(tqdm(loader, desc=f\"Task {t} | Epoch {epoch}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_cur = model(x)\n",
    "            ce_cur = criterion(logits_cur, y)\n",
    "            reg_loss = si.penalty() if t > 0 else 0.0\n",
    "\n",
    "            kd, ce_rep = 0.0, 0.0\n",
    "            if teacher is not None:\n",
    "                xr, yr = sample_memory(batch_size=max(1, BATCH_SIZE // 4))\n",
    "                if xr is not None:\n",
    "                    with torch.no_grad():\n",
    "                        t_logits = teacher(xr)\n",
    "                    s_logits = model(xr)\n",
    "                    ce_rep = criterion(s_logits, yr)\n",
    "                    kd = kd_loss(s_logits, t_logits, T=2.0)\n",
    "\n",
    "            loss = ce_cur + 0.5 * ce_rep + 0.5 * kd + reg_loss\n",
    "\n",
    "            old_params = [p.data.clone() for p in si.params]\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            si.update_w_after_step(old_params)\n",
    "\n",
    "            running_loss += float(ce_cur.detach().cpu())\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Batch {i}/{len(loader)} | CE: {ce_cur:.4f} | SI: {float(reg_loss):.4f} | REP: {float(ce_rep):.4f} | KD: {float(kd):.4f}\")\n",
    "\n",
    "        print(f\"Avg CE for Task {t}, Epoch {epoch+1}: {running_loss / len(loader):.4f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    si.end_task()\n",
    "    update_memory_from_loader(loader, per_class=MEMORY_MAX_PER_CLASS)\n",
    "\n",
    "    # Eval\n",
    "    for eval_t in range(t + 1):\n",
    "        acc = evaluate(model, test_loaders[eval_t], class_offset=eval_t * classes_per_task)\n",
    "        results[t][eval_t] = acc\n",
    "        print(f\"[Task {t}] Eval on Task {eval_t}: {acc:.2f}%\")\n",
    "\n",
    "    print(\"\\nAccuracy Summary After Task\", t)\n",
    "    for prev_task in range(t + 1):\n",
    "        print(f\"    Task {prev_task}: {results[t][prev_task]:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete. Results matrix:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
